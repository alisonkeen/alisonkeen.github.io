{
  "name": "AlisonKeen on GitHub",
  "tagline": "",
  "body": "In case anyone's curious what I have been working on... the short answer is 'OpenAustralia SA'. The long answer is as follows...\r\n\r\n### Thur 17-11-2016\r\nLittle bit of childcare. Managed to modify ruby scraper to read API to get only JSON list of *all* transcript TOC files, then generate html output with correct links so I can manually download missing files. Fixed some HTML/CSS to make the current very-basic auto-generated summaries (from the TOC files) sort of usable. \r\n\r\nTried working out how to get Morph.io to read the JSON response from the API and import the list of Hansard files into sqlite automatically... google failed me. Not sure which scraper engine will let me get raw JSON.\r\n\r\nStill haven't gotten it to detect new files properly, or email me when there are files to download.\r\n\r\n### Fri-Sun 11-13-11-2016\r\nIn desperation, tried getting the SA Hansard API working (details: [SA Hansard API Docs](https://parliament-api-docs.readthedocs.io/en/latest/south-australia/) ) Only to find that part 1 of 3 returns a JSON index of the filenames for each sitting day in a given year; parts 2 and 3 which supposedly allow downloads of xml files are borked (\"Server Error\"). This means the API is okay for generating links to manually download files, but can't automatically collect the files you are interested in. To put this into perspective: there are 959 daily XML table-of-contents files, and each of them has between 50 and 100ish fragments. Each fragment also has to be manually downloaded (possibly from a script-generated URL, but who's got time to manually download 4000-ish fragments!?)\r\n\r\nAlso confirmed that both wget and curl are blocked from downloading the xml files; gave up trying to do it automatically. \r\n\r\n### Thur 10-11-2016\r\nNo childcare. Therefore no progress.\r\n\r\n### Thur 03-11-2016\r\nAAAAAAAHHHHH I hate Frontpage and Sharepoint like never before... Today I discovered that the SA Hansard 'Daily XML' isn't actually a full transcript, it's just a table of contents which tells you which MPs spoke, but not what they actually _said_. The actual content is broken into another ten or fifteen fragmented xml files (per day), which are tricky to download, even manually. Hmph. \r\n\r\nIn other news, I registered a new domain name, and configured Apache on my VPS to serve it up - now I have a place to put up analysis of each day's transcripts. (If I ever get my hands on them. HRMMPH.) \r\n\r\n### Sat 29-10-2016 \r\nWeekend programming time, yay! \r\nDiscovered PhantomJS/Capybara/..., which solved one of my roadblocks - JavaScript dynamically generated content on the SA Hansard \"search\" (index of posted days) website. \r\nThen discovered that PhantomJS works great run from my VPS but doesn't load everything on Morph.io\r\n\r\n### Thur 20-10-2016 and 27-10-2016 \r\nNo occasional care available so no block of computer time\r\n\r\n### Thur 6-10-2016 and 13-10-2016 \r\nInterstate visiting family, no chance to do much\r\n\r\n### Thur 29-9-2016 \r\nBlackouts the night before meant no coding due to two kids home from school and occasional care. \r\n\r\n### Thur 22-9-2016\r\n\r\nWhile the toddler was in care (morning, 90mins): \r\n- looked up why @Henare 's federal Bills scraper is erroring and decided to ignore it for now - the arguments needed to make the URL work are really NOT scraper friendly!!\r\n- Attempted to fix the bugs in my own scraper so it exports CSV that can be imported straight into OpenAustralia's codebase. Members one seems to be working now?\r\n- Morph.io/ruby/popolo issue: I discovered that you can't output 'group' as a column in the data item to sqlite3 from ruby. Something doesn't like it. Need to look up another name... \r\n- Created the skeleton of a parser on Morph to build a Mechanize scraper - to grab available Hansard transcript dates. While I was there got distracted. In the SA Hansard search-page source it clearly had a form to subscribe to alerts ... but it took five minutes of hunting to find a small \"create an alert\" link down the bottom of the page. \r\n\r\nWhile he was asleep in the afternoon: \r\n- Created this, as a way of keeping track of progress \r\n- I can't find the parser to convert ausparl XML to openaustralia XML. I can't even remember the right names for the schema, I don't remember where it was, but I'm sure I saw one a few weeks ago!?\r\n\r\n\r\n### Thurs 15/9\r\n\r\nCreated three more nokogiri scrapers tweaked from the first one which match the input format for OpenAustralia closely enough that I managed to get it to import the list of MPs. Only to discover I needed three lists - a general 'members' one with almost no info, a 'senators' one and a 'representatives' one. \r\n\r\nGot them working by emptying the 'ministers' and 'shadow-ministers' csv files - the ministerial portfolios need to be parsed and written into, as well. Given this is auxiliary info, it can wait until after I have a basic grip on Mechanize. The section/line control needed to handle the start/end dates and portfolio info available on the members' pages is a bit fiddly.\r\n\r\n### Thur 8/9\r\n\r\nFinally plucked up the courage to knuckle down and learn how Morph works, and write my first scraper. You know... I think I might actually really like Morph, partly for the way the output APIs all Just Work... :D\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}