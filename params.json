{
  "name": "AlisonKeen on GitHub",
  "tagline": "",
  "body": "In case anyone's curious what I have been working on, The long answer is as follows...\r\n\r\n### Thurs 15 Dec\r\nfew hours childcare, but because my older child is home from school, got essentially nothing done. \r\nDecided to try installing OpenAustralia dev environment using Vagrant. \r\nAttempt #1: \r\n- Tried on VPS: I do most dev on VPS to avoid installing random stuff and bricking my laptop, because if my laptop dies, and it takes days or weeks to resurrect, I am *stuck*. I can kill and replace a VPS in minutes. \r\n- Discovered that the 'vagrant up' script needs VirtualBox. Ohhh.... so it fires up a virtual machine with a GUI, not a command-line thing like docker. Right. Uninstalled vagrant from (now broken) main VPS. \r\n\r\nAttempt #2: \r\n - Attempted to install Vagrant on local machine. Chucked errors over invalid CUPS install (we don't own a printer; I don't know what this has to do with anything). \r\n - Finally solved CUPS issues. Discovered I still need to install something to run VMware image. \r\n - Attempted to install VirtualBox; it attempted to brick my computer by turning off SecureBoot (needed for Windows dual-boot, which I would like to still work). \r\n - According to [Ask Ubuntu](https://askubuntu.com/questions/760671/could-not-load-vboxdrv-after-upgrade-to-ubuntu-16-04-and-i-want-to-keep-secur), I need to sign the VirtualBox drivers so they work on my machine. I do *not* have time for this @#$^^. \r\n\r\n\r\n### Weekend 8-10 Dec\r\nTried writing up Cuttlefish how-to but got stuck. Needs to be in Rails friendly syntax. Problem A: I don't speak Rails. Problem B: I can figure out how to install rails, but no idea what the hell you do next, because unlike Apache, it doesn't appear to have a nice simple document-root location where you just load files from.... I give up. For now, anyway.\r\n\r\n### Sat 3 Dec \r\nHad a quick look around at different states to check if there were any who were playing nice(r) than SA. A bit shocked that at least three states *only* have PDFs up, and no or very basic search. Discovered Ruby's pdf-reader gem that might be worth playing with later... \r\n\r\nCreated a Config class, re-read ruby syntax guidelines (I swear they make more sense every week). \r\n\r\nSet up a cronjob on local server to run scrapers (for now). \r\n\r\nSent my first email using Cuttlefish.io YAYY!!!!! now to fork @mlandauer's cuttlefish repo and add some docs on what code works from ruby-2.3 to actually talk to it (hint: needs start_tls_auto called before start). If you want to see what the code looks like, have a look at email_helper.rb . \r\n\r\nIt'll be a while before there's any sort of list I can add anyone to though... There's no backend database code at all, the current generated summaries are just precompiled flat-files. \r\n\r\nThe summary php files are only php so I can be lazy and include page formatting (header,footer) without needing to add whole html blocks manually every time. I still see them as a precurser to being able to generate a properly-formated parlparse xml output - the generate_summary code is currently *very* basic and doesn't traverse the tree at all. \r\n\r\n### 24 Nov, 1st Dec, 8 Dec\r\n(No childcare, booked out until 15 Dec)\r\n\r\n### Weekend 18 to 20 Nov and 25 to 27 Nov\r\nWorking on pulling basic info out of XML TOC files, so at least I can get the gist of whether there is anything of interest from a particular day's transcripts. Have given up trying to merge fragments with tocs to create a parlparse-friendly xml output (for now) - too many fragments to manually download. Maybe in a year or two, if the site is overhauled and becomes friendlier, otherwise write script to either generate links for inserting fragments of interest or manually download fragments for one day at a time. (Hey, if I can manually download 949 TOC's to make them mine-able, surely I can do 200 fragments for the most recent sitting day. Sometimes).\r\n\r\nAlso improved codebase significantly to create a webhook handler and figured out how to code and call ruby modules, to make code much more reusable. \r\n\r\n... Then lost motivation. I can write what needs doing on paper, just can't... write ... the actual... code on the screen\r\n\r\n... Spent too much time reading Twitter again instead of getting on with life. It's hard to concentrate on much when a toddler or child interrupts every 10-15 minutes (on average)\r\n\r\n### Thur 17-11-2016\r\nLittle bit of childcare. Managed to modify ruby scraper to read API to get only JSON list of *all* transcript TOC files, then generate html output with correct links so I can manually download missing files. Fixed some HTML/CSS to make the current very-basic auto-generated summaries (from the TOC files) sort of usable. \r\n\r\nTried working out how to get Morph.io to read the JSON response from the API and import the list of Hansard files into sqlite automatically... google failed me. Not sure which scraper engine will let me get raw JSON.\r\n\r\nStill haven't gotten it to detect new files properly, or email me when there are files to download.\r\n\r\n### Fri-Sun 11-13-11-2016\r\nIn desperation, tried getting the SA Hansard API working (details: [SA Hansard API Docs](https://parliament-api-docs.readthedocs.io/en/latest/south-australia/) ) Only to find that part 1 of 3 returns a JSON index of the filenames for each sitting day in a given year; parts 2 and 3 which supposedly allow downloads of xml files are borked (\"Server Error\"). This means the API is okay for generating links to manually download files, but can't automatically collect the files you are interested in. To put this into perspective: there are 959 daily XML table-of-contents files, and each of them has between 50 and 100ish fragments. Each fragment also has to be manually downloaded (possibly from a script-generated URL, but who's got time to manually download 40,000-ish fragments!?)\r\n\r\nAlso confirmed that both wget and curl are blocked from downloading the xml files; gave up trying to do it automatically. \r\n\r\n### Thur 10-11-2016\r\nNo childcare. Therefore no progress.\r\n\r\n### Thur 03-11-2016\r\nAAAAAAAHHHHH I hate Frontpage and Sharepoint like never before... Today I discovered that the SA Hansard 'Daily XML' isn't actually a full transcript, it's just a table of contents which tells you which MPs spoke, but not what they actually _said_. The actual content is broken into another ten or fifteen fragmented xml files (per day), which are tricky to download, even manually. Hmph. \r\n\r\nIn other news, I registered a new domain name, and configured Apache on my VPS to serve it up - now I have a place to put up analysis of each day's transcripts. (If I ever get my hands on them. HRMMPH.) \r\n\r\n### Sat 29-10-2016 \r\nWeekend programming time, yay! \r\nDiscovered PhantomJS/Capybara/..., which solved one of my roadblocks - JavaScript dynamically generated content on the SA Hansard \"search\" (index of posted days) website. \r\nThen discovered that PhantomJS works great run from my VPS but doesn't load everything on Morph.io\r\n\r\n### Thur 20-10-2016 and 27-10-2016 \r\nNo occasional care available so no block of computer time\r\n\r\n### Thur 6-10-2016 and 13-10-2016 \r\nInterstate visiting family, no chance to do much\r\n\r\n### Thur 29-9-2016 \r\nBlackouts the night before meant no coding due to two kids home from school and occasional care. \r\n\r\n### Thur 22-9-2016\r\n\r\nWhile the toddler was in care (morning, 90mins): \r\n- looked up why @Henare 's federal Bills scraper is erroring and decided to ignore it for now - the arguments needed to make the URL work are really NOT scraper friendly!!\r\n- Attempted to fix the bugs in my own scraper so it exports CSV that can be imported straight into OpenAustralia's codebase. Members one seems to be working now?\r\n- Morph.io/ruby/popolo issue: I discovered that you can't output 'group' as a column in the data item to sqlite3 from ruby. Something doesn't like it. Need to look up another name... \r\n- Created the skeleton of a parser on Morph to build a Mechanize scraper - to grab available Hansard transcript dates. While I was there got distracted. In the SA Hansard search-page source it clearly had a form to subscribe to alerts ... but it took five minutes of hunting to find a small \"create an alert\" link down the bottom of the page. \r\n\r\nWhile he was asleep in the afternoon: \r\n- Created this, as a way of keeping track of progress \r\n- I can't find the parser to convert ausparl XML to openaustralia XML. I can't even remember the right names for the schema, I don't remember where it was, but I'm sure I saw one a few weeks ago!?\r\n\r\n\r\n### Thurs 15/9\r\n\r\nCreated three more nokogiri scrapers tweaked from the first one which match the input format for OpenAustralia closely enough that I managed to get it to import the list of MPs. Only to discover I needed three lists - a general 'members' one with almost no info, a 'senators' one and a 'representatives' one. \r\n\r\nGot them working by emptying the 'ministers' and 'shadow-ministers' csv files - the ministerial portfolios need to be parsed and written into, as well. Given this is auxiliary info, it can wait until after I have a basic grip on Mechanize. The section/line control needed to handle the start/end dates and portfolio info available on the members' pages is a bit fiddly.\r\n\r\n### Thur 8/9\r\n\r\nFinally plucked up the courage to knuckle down and learn how Morph works, and write my first scraper. You know... I think I might actually really like Morph, partly for the way the output APIs all Just Work... :D\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}