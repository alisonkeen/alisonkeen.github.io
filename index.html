<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>AlisonKeen on GitHub by alisonkeen</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>AlisonKeen on GitHub</h1>
        <p></p>


        <p class="view"><a href="https://github.com/alisonkeen">View My GitHub Profile</a></p>

      </header>
      <section>
        <p>In case anyone's curious what I have been working on... the short answer is 'OpenAustralia SA'. The long answer is as follows...</p>

<h3>
<a id="thur-17-11-2016" class="anchor" href="#thur-17-11-2016" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Thur 17-11-2016</h3>

<p>Little bit of childcare. Managed to modify ruby scraper to read API to get only JSON list of <em>all</em> transcript TOC files, then generate html output with correct links so I can manually download missing files. Fixed some HTML/CSS to make the current very-basic auto-generated summaries (from the TOC files) sort of usable. </p>

<p>Tried working out how to get Morph.io to read the JSON response from the API and import the list of Hansard files into sqlite automatically... google failed me. Not sure which scraper engine will let me get raw JSON.</p>

<p>Still haven't gotten it to detect new files properly, or email me when there are files to download.</p>

<h3>
<a id="fri-sun-11-13-11-2016" class="anchor" href="#fri-sun-11-13-11-2016" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Fri-Sun 11-13-11-2016</h3>

<p>In desperation, tried getting the SA Hansard API working (details: <a href="https://parliament-api-docs.readthedocs.io/en/latest/south-australia/">SA Hansard API Docs</a> ) Only to find that part 1 of 3 returns a JSON index of the filenames for each sitting day in a given year; parts 2 and 3 which supposedly allow downloads of xml files are borked ("Server Error"). This means the API is okay for generating links to manually download files, but can't automatically collect the files you are interested in. To put this into perspective: there are 959 daily XML table-of-contents files, and each of them has between 50 and 100ish fragments. Each fragment also has to be manually downloaded (possibly from a script-generated URL, but who's got time to manually download 4000-ish fragments!?)</p>

<p>Also confirmed that both wget and curl are blocked from downloading the xml files; gave up trying to do it automatically. </p>

<h3>
<a id="thur-10-11-2016" class="anchor" href="#thur-10-11-2016" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Thur 10-11-2016</h3>

<p>No childcare. Therefore no progress.</p>

<h3>
<a id="thur-03-11-2016" class="anchor" href="#thur-03-11-2016" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Thur 03-11-2016</h3>

<p>AAAAAAAHHHHH I hate Frontpage and Sharepoint like never before... Today I discovered that the SA Hansard 'Daily XML' isn't actually a full transcript, it's just a table of contents which tells you which MPs spoke, but not what they actually <em>said</em>. The actual content is broken into another ten or fifteen fragmented xml files (per day), which are tricky to download, even manually. Hmph. </p>

<p>In other news, I registered a new domain name, and configured Apache on my VPS to serve it up - now I have a place to put up analysis of each day's transcripts. (If I ever get my hands on them. HRMMPH.) </p>

<h3>
<a id="sat-29-10-2016" class="anchor" href="#sat-29-10-2016" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Sat 29-10-2016</h3>

<p>Weekend programming time, yay! 
Discovered PhantomJS/Capybara/..., which solved one of my roadblocks - JavaScript dynamically generated content on the SA Hansard "search" (index of posted days) website. 
Then discovered that PhantomJS works great run from my VPS but doesn't load everything on Morph.io</p>

<h3>
<a id="thur-20-10-2016-and-27-10-2016" class="anchor" href="#thur-20-10-2016-and-27-10-2016" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Thur 20-10-2016 and 27-10-2016</h3>

<p>No occasional care available so no block of computer time</p>

<h3>
<a id="thur-6-10-2016-and-13-10-2016" class="anchor" href="#thur-6-10-2016-and-13-10-2016" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Thur 6-10-2016 and 13-10-2016</h3>

<p>Interstate visiting family, no chance to do much</p>

<h3>
<a id="thur-29-9-2016" class="anchor" href="#thur-29-9-2016" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Thur 29-9-2016</h3>

<p>Blackouts the night before meant no coding due to two kids home from school and occasional care. </p>

<h3>
<a id="thur-22-9-2016" class="anchor" href="#thur-22-9-2016" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Thur 22-9-2016</h3>

<p>While the toddler was in care (morning, 90mins): </p>

<ul>
<li>looked up why <a href="https://github.com/Henare" class="user-mention">@Henare</a> 's federal Bills scraper is erroring and decided to ignore it for now - the arguments needed to make the URL work are really NOT scraper friendly!!</li>
<li>Attempted to fix the bugs in my own scraper so it exports CSV that can be imported straight into OpenAustralia's codebase. Members one seems to be working now?</li>
<li>Morph.io/ruby/popolo issue: I discovered that you can't output 'group' as a column in the data item to sqlite3 from ruby. Something doesn't like it. Need to look up another name... </li>
<li>Created the skeleton of a parser on Morph to build a Mechanize scraper - to grab available Hansard transcript dates. While I was there got distracted. In the SA Hansard search-page source it clearly had a form to subscribe to alerts ... but it took five minutes of hunting to find a small "create an alert" link down the bottom of the page. </li>
</ul>

<p>While he was asleep in the afternoon: </p>

<ul>
<li>Created this, as a way of keeping track of progress </li>
<li>I can't find the parser to convert ausparl XML to openaustralia XML. I can't even remember the right names for the schema, I don't remember where it was, but I'm sure I saw one a few weeks ago!?</li>
</ul>

<h3>
<a id="thurs-159" class="anchor" href="#thurs-159" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Thurs 15/9</h3>

<p>Created three more nokogiri scrapers tweaked from the first one which match the input format for OpenAustralia closely enough that I managed to get it to import the list of MPs. Only to discover I needed three lists - a general 'members' one with almost no info, a 'senators' one and a 'representatives' one. </p>

<p>Got them working by emptying the 'ministers' and 'shadow-ministers' csv files - the ministerial portfolios need to be parsed and written into, as well. Given this is auxiliary info, it can wait until after I have a basic grip on Mechanize. The section/line control needed to handle the start/end dates and portfolio info available on the members' pages is a bit fiddly.</p>

<h3>
<a id="thur-89" class="anchor" href="#thur-89" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Thur 8/9</h3>

<p>Finally plucked up the courage to knuckle down and learn how Morph works, and write my first scraper. You know... I think I might actually really like Morph, partly for the way the output APIs all Just Work... :D</p>
      </section>
      <footer>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
